# Linear Maps and Matrices

## The Matrix of a Linear Map

Let $\alpha:V \rightarrow W$ be a $F-$linear map between the (finite dimensional) vector spaces $V$ and $W$. (Recall that this means that $\alpha$  respects vector space structure, i.e. $\alpha (a \cdot v + b \cdot w) = a \cdot \alpha (v) + b \cdot \alpha (w)$ for all $a, b \in F$ and $v, w \in V$.)

Let $v_{1},...,v_{n}$ and $w_{1}, ..., w_{n}$ be bases of $V$ and $W$, respectively. Then there exist unique elements $a_{ij}\in F, 1 \leq i \leq m, 1 \leq y \leq n$, such that

$$
\alpha (v_{j})= \sum\limits_{i = 1}^{m}a_{ij}w_{i}.
$$

The $m \times n$ matrix

$$
\begin{align}
A = (a_{ij})=
\begin{bmatrix} a_{11} & \cdots & a_{1n}\\\vdots & \ddots & \vdots \\ a_{m1} & \cdots & a_{mn} \end{bmatrix}
\end{align}
$$

Is the matrix of $\alpha$ with respect to these bases.

If $V = W$ and $v_{i}= w_{i}$, we say also that $A$ is the matrix of $alpha$ with respect to the basis $v_{1}, ..., v_{n}$.

If $v_{1}', ..., v_{n}'$ and $w_{1}', ..., w_{m}'$ are other bases of $V$ and $W$ respectively, then there are unique elements $s_{ij}$ and $t_{ij}$ of $F$, such that

$$
\begin{align}
v_{i}' = \sum\limits_{h = 1}^{n}s_{hi}v_{h} & & \text{and} & & w_{i}' = \sum\limits_{h = 1}^{m}t_{hi}w_{h}.
\end{align}
$$

Analogously there are $s_{ij}', t_{ij}'\in F$, such that

$$
\begin{align}
v_{i} = \sum\limits_{h = 1}^{n}s_{hi}'v_{h}' && \text{and} && w_{i}= \sum\limits_{h = 1}^{m}t_{hi}'w_{h}'.
\end{align}
$$

Inserting the first set of equations to the second, then the linear independence of $v_{1}, ..., v_{n}$ and of $w_{1}, ..., w_{m}$ implies

$$
\begin{align}
\sum\limits_{h = 1}^{n}s_{lh}\cdot s_{hi}' = \sum\limits_{h = 1}^{m}t_{lh}\cdot t_{hi}' =
\begin{cases}1 \; i = l \\
0 \; i \neq l,
\end{cases}
\end{align}
$$

or, in other words, the matrices $S = (s_{ij})$ and $T = (t_{ij})$ are invertible with inverses $S^{-1}= (s_{ij}')$ and $T^{-1}= (t_{ij}')$, respectively.

We use this to compute the matrix $A' = (a'_{ij})$ of the linear map $\alpha$ with respect to the bases $v_{1}', ..., v_{n}'$ and $w_{1}', ..., w_{m}'$. We have ^045152

$$
\begin{align}
\alpha (v'_{i}) & = \alpha (\sum\limits_{h = 1}^{n}s_{hi}v run{h})= \sum\limits_{h = 1}^{n}s_{hi}\alpha (v_{h})\\
& = \sum\limits_{h = 1}^{n}s_{hi}\sum\limits_{l = 1}^{m}a_{lh}w_{l}\\
& = \sum\limits_{h = 1}^{n}s_{hi}\sum\limits_{l = 1}^{m}a_{lh}\sum\limits_{j = 1}^{m}t'_{jl}w'_{j}\\
& = \sum\limits_{j = 1}^{m}\left(\sum\limits_{l = 1}^{m}t'_{jl}\left(\sum\limits_{h = 1}a_{lh}s_{hi}\right)\right)\cdot w_{j}'.
\end{align}
$$

Therefore, $a'_{ij}= \sum\limits_{l = 1}^{m}t'_{jl}\left(\sum\limits_{h = 1}^{n}a_{lh}s_{hi}\right)$, and this is the $ij-$coefficient of the matrix $T^{-1}\cdot A \cdot S$. In other words,

$$
A' = T^{-1}\cdot A \cdot S
$$

## Characteristic Polynomial

For an $n \times n$ matrix, its characteristic polynomial is defined as

$$
P_{A}(T) = \det (T \cdot I_{n}-A).
$$

The characteristic polynomial of $\alpha:V \rightarrow V$ is given by $P_{\alpha}:= P_{A(T)}$ where $A$ is a matrix of $\alpha$ with respect to the source basis of $V$. The characteristic polynomial does not depend on its basis. In other words, from the above,

$$
A' = S^{-1}\cdot A \cdot S.
$$

Given the matrix

$$
\begin{align}
\det
\begin{bmatrix}
A & D \\ O & B
\end{bmatrix}
= \det (A)\cdot \det(B)
\end{align}
$$

Where $A$ is an $m \times m$ matrix, $B$  is an $n \times n$ matrix, and $D$ is any $m \times n$ matrix.

## Invariant Subspace

Given a linear map $\alpha:V \rightarrow V$, an $\alpha$-invariant subspace of $V$ is a subspace $W$ of $V$ such that $\alpha (w)\in W$ for all $w \in W$.

In words, $\alpha$ maps $W$ to itself.

**Example:** If $\lambda$ is an eigenvalue of $\alpha$ and $w \in W$ is an eigenvector, then $W = F \cdot w = \{\mu w \mid\mu \in F\}$ is $\alpha$-invariant.

Since $\alpha (\mu \cdot w)\overset{\alpha}{=}\mu\cdot \alpha (w)\overset{w}{=}\mu \cdot (\lambda \cdot w)= (\mu \cdot \lambda)\cdot w \in F \cdot w$. The whole eigenspace $E_{\lambda}$ for $\lambda$ is $\alpha$-invariant. $w \in E_{\lambda}$ then $\alpha (w)= \lambda \cdot w$ and $\lambda w \in E_{\lambda}$ again since $\alpha (\lambda w)\overset{\alpha}{=}\lambda \alpha (w)\overset{w}{=}\lambda^{2}w = \lambda (\lambda w)$.

Assume $\alpha$ is again a linear map, then

$$W \leq V$$ 

is $\alpha$-invariant, with $w_{1}, ..., w_{e}$ basis of $W$. We can extend this to a basis of $V$.

The smallest $\alpha-$invariant subspace $w$ containing a non-zero vector $v \in V$ must contain $\alpha (v), \; \alpha (\alpha (v))= \alpha^{2}(v), ..., \alpha^{n}(v)$. This leads to the claim that $W =$ subspace generated by $\alpha^{n}(v), \; n = 0, 1, 2, ...$ is $\alpha-$invariant.

Since every $w \in W$ can be written as

$$
w = \sum\limits_{j = 0}^{m}b_{j}\alpha (v)^{j}, \; b_{j}\in F.
$$

This implies

$$
\alpha (w)\overset{\alpha}{=}\sum\limits_{j = 0}^{m}b_{j}\alpha (\alpha^{j}(v))= \sum\limits_{j = 0}^{m}b_{j}\alpha^{j + 1}(v)\in W
$$

$\dim W < \infty$ as $W \subseteq V$ and $\dim V < \infty$.

Choose $l$ minimal such that

$$
v_{1},\alpha (v)_{1},\alpha^{2}(v)_{1}, ..., \alpha^{l}(v)_{1}
$$

are linearly independent. We claim the above is a basis. We already showed these vectors are linearly independent, we annual have to show they generate $W$. It is enough to show that $\alpha^{n}(v)$ is in the linear span of $v_{1}\alpha (v)_{1}, ..., \alpha^{l-1}(v)_{1}$ for all $n = 0, 1, 2, ...$. By induction on $n$, $0 \leq n \leq l-1$ and we show it for $\alpha^{(l-1)+ i}(v)$. The induction beginning is clear, so let $i \geq 1$. By the induction hypothesis, we know that $v, \alpha, ..., \alpha^{(l + 1)+ i-1}(v)$ are in the span of $v, \alpha (v), ..., \alpha^{l-1}(v)$. On the other hand by our assumption on $l$ there exists $a_{0}, ..., a_{l-1}$, such that $\alpha^{l}(v)+ \sum\limits_{i = 0}^{l-1}a_{1}\cdot a^{i}(v)= 0$, or, equivalently

$$
\alpha^{l}(v) = -a_{0}\cdot v-a_{1}\cdot \alpha(v)+...-a_{l-1}\cdot \alpha^{l + 1}(v),
$$

and so $\alpha^{l-1 + i}(v)= \sum\limits_{j = 0}^{l-1}-aj \cdot \alpha^{i-1 + j}(v)$ is a linear combination of the vectors $v, \alpha (v), \alpha^{2}(v),...,\alpha^{l-1}(v)$. 

The linear map  $\alpha$ defines by restriction an $F-$linear map from $W (\alpha, v)$ so itself. Use the definition for $\alpha^{l}(v)$ from above, we see that this linear map has the following $l \times l$ matrix with respect to the basis $v, \alpha (v), ..., \alpha^{l-1}(v)$:

$$
\begin{align}
B =
\begin{bmatrix}
0 & 0 & 0 &... & 0 & -a_{0} \\ 1 & 0 & 0 &... & 0 & -a_{1} \\ 0 & 1 & 0 & ... & 0 & -a_{2} \\ \vdots & \ddots & & & & \vdots \\ \vdots & & \ddots & & & \vdots \\ 0 & & &... & 1& -a_{l-1} \end{bmatrix}
\end{align}
$$