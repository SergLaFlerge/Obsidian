# The Cayley-Hamilton Theorem

##  The Characteristic Polynomial

Let $A$ be an $n \times n$ matrix. Its characteristic polynomial is defined as the polynomial

$$
\det (TI_{n}-A)
$$

In the variable $T$, where $I_{n}$ denotes the $n \times n$ identity matrix. Recall that similar matrices have the same characteristic polynomial: If $S$ is an invertible matrix then

$$
P_{S^{-1}\cdot A \cdot S}(T) = P_{A}(T).
$$

Using this, we can define the *characteristic polynomial* of an $F-$linear map $\alpha$ of a finite dimensional space into itself. If $v_{1}, ..., v_{n}$ is a basis of $V$ and $A$ is the matrix of $\alpha$ with respect to this basis we set

$$
P_{\alpha}(T):= P_{A}(T).
$$

We can see that this does not depend on the choice of basis. If $A'$ is another matrix of $\alpha$ with respect to a different basis, by the [[Linear Maps and Matrices#^045152|formula]] of section 1, we have $A' = S^{-1}\cdot A \cdot S$, where $S = (s_{ij})$ is the base change matrix. From the above remarks, it is clear that $P_{A}(T)= P_{A'}(T)$ and so $P_{\alpha}(T)$ does not depend on the choice of basis of $V$.

**Remark:** Let $A$ be an $n \times n$ matrix and $\alpha_{A}:F^{n}\rightarrow F^{n}$ be the linear map $v \mapsto A \cdot v$. Then $A$ is the matrix of $\alpha_{A}$ with respect to the standard basis, and so $P_{\alpha A}(T)= P_{A}(T)$, i.e. the characteristic polynomial of the linear map defined by a matrix $A$ is equal to the characteristic polynomial of the matrix $A$

#### Example

Let $B\in M_{l \times l}(F), \; C \in M_{l \times (n-l)(F)}$, and $D \in M_{(n-l)\times (n-l)}(M)$, then $A$ is an $n \times n$ matrix,

$$
\begin{align}
A =
\begin{bmatrix}
B & C \\ O_{(n-l)\times l} & D
\end{bmatrix},
\end{align}
$$

and 

$$P_{A}(T)= P_{B}(T)\cdot P_{D}(T).$$

## Characteristic Polynomial and the Invariant Subspace Generated by a Vector

Let $\alpha$ be an $F-$linear map and $v \neq 0$ be a vector in $V$. Recall that there is an $l  \geq 1$ such that $v, \alpha (v), \alpha^{2}(v), ..., \alpha^{l-1}(v)$ are a basis of the smallest $\alpha-$invariant subspace $W (\alpha, v)$ of $V$ which contains $v$, and that $v, \alpha (v), \alpha^{2}(v), ..., \alpha^{l-1}(v), \alpha (v)$ are linearly dependent. In particular, there are $a_{0}, ..., a_{l-1}\in F$ such that

$$
\alpha^{l}(v)+ \sum\limits_{i = 0}^{l-1}a_{i}\alpha^{i}(v)= 0
$$

This restriction of $\alpha$ to $W$ has a matrix

$$
\begin{align}
B =
\begin{bmatrix}
0 & 0 & 0 &... & 0 & -a_{0} \\ 1 & 0 & 0 &... & 0 & -a_{1} \\ 0 & 1 & 0 & ... & 0 & -a_{2} \\ \vdots & \ddots & & & & \vdots \\ \vdots & & \ddots & & & \vdots \\ 0 & & &... & 1& -a_{l-1} \end{bmatrix}
\end{align}
$$

with respect to this basis. We claim that

$$
P_{B}(T) = T^{l}+ a_{l-1}T^{l-1}+... + a_{1}T + a_{0}.
$$

This is easily checked for $l = 1$ and $l = 2$, and the rest follow by induction using expansion by minors along the first row.

We can expand this basis of $W$ into a basis of $V$:

$$
v, \alpha (v), ..., \alpha^{l-1}(v), v_{l + 1}, ... , v_{n}.
$$

with $v_{l + 1}, ..., v_{n}\in V$ being vectors "filling in" the missing columns.

With respect to this basis, $\alpha$ is represented by the matrix

$$
\begin{align}
A =
\begin{bmatrix}
B & C \\ O_{(n-l)\times l} & D,
\end{bmatrix}
\end{align}
$$

where $B, C,D$ are defined as in example 2. Hence by example 2, we have

$$
P_{\alpha}(T) = P_{A}(T)= P_{B}(T)\cdot P_{D}(T)= \left(T^{l}+ \sum\limits_{i = 0}^{l-1}a_{i}T^{i}\right) \cdot P_{D}(T).
$$

## Matrix Polynomials

Let $f (T)= \sum\limits_{i = 0}^{n}a_{i}T^{i}$ be a polynomial in one variable $T$ over the field $F$, and $A$ an$n \times n$ matrix over $F$. The matrix $f (A)$ is then defined by evaluating the polynomial $f (T)$ at $A$, i.e.

$$
f (A):= \sum\limits_{i = 0}^{n}a_{i}\cdot A^{i}= a_{0}\cdot I_{n}+ a_{1}\cdot A+ a_{2}\cdot A^{2}+ \cdots + a_{n}\cdot A^{n},
$$
where $A^{0}= I_{n}$ is the $n \times n$ identity matrix. This is the jumping-off point for the Cayley-Hamilton Theorem, but before that, we need the following lemma:

#### Lemma

Let $f (T)= \sum\limits_{i = 0}^{m}a_{i}T^{i}$ and $g (T)= \sum\limits_{i = 0}^{n}b_{i}T^{i}$ be two polynomials, and let $A$ be an $n \times n$ matrix. Then:
1. The matrices $f (A)$ and $g (A)$ commute with each other, i.e. $$f (A)\cdot g (A)= g (A)\cdot f (A).$$
2. If $W \subset F^{n}$ is $A-$invariant, then it is also $f (A)-$invariant.

Similarly, we can define the polynomial of a linear map $\alpha:V \rightarrow V$. If $f (T)= \sum\limits_{i = 0}^{n}a_{i}T^{i}$ then

$$
f (\alpha) = \sum\limits_{i = 0}^{n}s_{i}\alpha^{i},
$$

where $\alpha^{0}= 1_{V}$ is understood. For instance, if $f (T)= 3T^{3}+ 10T^{2}-1$ then $f (\alpha)$ is the $F-$linear map

$$
v \mapsto 3 \cdot \alpha (\alpha (\alpha (v)))+ 10 \cdot \alpha (\alpha (v))-v.
$$

Then we also have the analog of the lemma above,

$$
f (\alpha)\circ g(\alpha)= g (\alpha)\circ f (\alpha),
$$

for all polynomials $f (T),\; g (T)$, and if $W \subseteq V$ is an $\alpha-$invariant subspace, then it is also an $f (\alpha)-$invariant subspace.

## The Cayley- Hamilton Theorem

Let $\alpha:V \rightarrow V$ be a linear map. Then

$$
P_{\alpha}(\alpha) = 0.
$$

In particular, for $n \times n$ matrices, we have $P_{A}(A)= 0$. This is, a matrix evaluated on its own characteristic polynomial gives the zero matrix. The proof is in the class notes.

By this theorem, given a linear map $\alpha$ of a vector space $V$ over a field $F$, there exists a polynomial $f (T)\in F [T]$, with degree $\geq 1$, such that $f (\alpha)= 0$. We say that $\alpha$ is a zero of $f (T)$.

#### The Minimal Polynomial

Let $\alpha:V \rightarrow V$ be an $F-$linear map. The *minimal polynomial* of $\alpha$, denoted $M_{\alpha}(T)\in F [T]$, is the monic polynomial $f (T)\in F [T]$ of smallest positive degree, such that $f (\alpha)= 0$. (Recall that a monic polynomial is a polynomial whose leading coefficient is $1$.)

The minimal polynomial of an $n \times n$ matrix $A$ is defined to be the minimal polynomial of the map

$$
\begin{align}
\alpha_{A}:F^{n}&\rightarrow F^{n}\\
x & \mapsto Ax.
\end{align}
$$

Observe that the minimal polynomial $M_{\alpha}(T)$ of $\alpha$ divides *all* polynomials $g (T)$ which satisfy $g (\alpha)= 0$. In fact, by the [[2.2- The Euclidean Algorithm|Euclidean algorithm]], one can write, given $g (T)\in F (T)$,

$$
g (T)= h (T)\cdot M_{\alpha}(T)+ r (T)
$$

with $h (T), \; r (T)\in F [T]$ and $\deg r(T)< \deg M_{\alpha}(T)$. If now $g (\alpha)= 0$, then the equation implies $r (\alpha)= g (\alpha)-h (\alpha)\cdot M_{\alpha}(\alpha)= 0$, and so $r (T)= 0$ since $M_{\alpha}(T)$ is the polynomial of smallest degree, which by default has $\alpha$ as a zero.

In particular, by the Cayley-Hamilton Theorem, the minimal polynomial divides the characteristic polynomial, but the two may in fact differ. For example, the characteristic polynomial of the $n \times n$ identity matrix is $(T-1)^{n}$, but the minimal polynomial of this matrix is $T-1$.